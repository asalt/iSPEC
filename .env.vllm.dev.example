# iSPEC support assistant (vLLM dev)
#
# Usage:
#   cp iSPEC/.env.vllm.dev.example iSPEC/.env.vllm.dev
#   set -a && . iSPEC/.env.vllm.dev && set +a
#
# Start vLLM separately (example):
#   export VLLM_USE_V1=0
#   export VLLM_ATTENTION_BACKEND=TRITON_ATTN
#   vllm serve allenai/Llama-3.1-Tulu-3-8B --host 127.0.0.1 --port 8000 --dtype float16 \
#     --max-model-len 8192 --max-num-seqs 8 --max-num-batched-tokens 4096 --gpu-memory-utilization 0.95 \
#     --enable-chunked-prefill

ISPEC_ASSISTANT_PROVIDER=vllm
ISPEC_ASSISTANT_NAME=iSPEC

# Prompt building / state
ISPEC_ASSISTANT_HISTORY_LIMIT=20
ISPEC_ASSISTANT_MAX_PROMPT_TOKENS=6000
ISPEC_ASSISTANT_SUMMARY_MAX_CHARS=2000

# Allow on-demand DB tool calls from the model.
ISPEC_ASSISTANT_MAX_TOOL_CALLS=2

# Optional: point to a repo-tracked prompt file (easier than editing env strings)
# ISPEC_ASSISTANT_SYSTEM_PROMPT_PATH=docs/prompts/ispec-system.txt
# ISPEC_ASSISTANT_SYSTEM_PROMPT_EXTRA_PATH=docs/prompts/ispec-extra.txt

# vLLM (OpenAI-compatible server)
ISPEC_VLLM_URL=http://127.0.0.1:8000
ISPEC_VLLM_MODEL=allenai/Llama-3.1-Tulu-3-8B

# vLLM can be slow on first load / long prompts; keep this high for dev.
ISPEC_VLLM_TIMEOUT_SECONDS=300
