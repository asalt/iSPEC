# iSPEC support assistant (vLLM dev)
#
# Usage:
#   cp iSPEC/.env.vllm.dev.example iSPEC/.env.vllm.dev
#   set -a && . iSPEC/.env.vllm.dev && set +a
#
# Start vLLM separately (example):
#   export VLLM_USE_V1=0
#   export VLLM_ATTENTION_BACKEND=TRITON_ATTN
#   vllm serve allenai/Llama-3.1-Tulu-3-8B --host 127.0.0.1 --port 8000 --dtype float16 \
#     --max-model-len 8192 --max-num-seqs 8 --max-num-batched-tokens 4096 --gpu-memory-utilization 0.95 \
#     --enable-chunked-prefill

ISPEC_ASSISTANT_PROVIDER=vllm
ISPEC_ASSISTANT_NAME=iSPEC
ISPEC_ASSISTANT_TOOL_PROTOCOL=openai

# Prompt building / state
ISPEC_ASSISTANT_HISTORY_LIMIT=20
ISPEC_ASSISTANT_MAX_PROMPT_TOKENS=6000
ISPEC_ASSISTANT_SUMMARY_MAX_CHARS=2000

# Allow on-demand DB tool calls from the model.
ISPEC_ASSISTANT_MAX_TOOL_CALLS=4
ISPEC_ASSISTANT_SELF_REVIEW=1
# Optional (experimental): return two draft answers and prompt the user to pick one.
# ISPEC_ASSISTANT_COMPARE_MODE=1
# Optional (vLLM-only): add a tiny KEEP/REWRITE decider before rewriting (reduces review tokens when draft is already good).
# ISPEC_ASSISTANT_SELF_REVIEW_DECIDER=1
# Optional: model sampling temperature (0â€“2). Lower is more deterministic.
# ISPEC_ASSISTANT_TEMPERATURE=0
# Optional (dev-only): enable repo code browsing tools (repo_search/repo_list_files/repo_read_file).
# ISPEC_ASSISTANT_ENABLE_REPO_TOOLS=1
# ISPEC_ASSISTANT_REPO_ROOT=/home/alex/tools/ispec-full

# Optional: point to a repo-tracked prompt file (easier than editing env strings)
# ISPEC_ASSISTANT_SYSTEM_PROMPT_PATH=docs/prompts/ispec-system.txt
# ISPEC_ASSISTANT_SYSTEM_PROMPT_EXTRA_PATH=docs/prompts/ispec-extra.txt

# vLLM (OpenAI-compatible server)
ISPEC_VLLM_URL=http://127.0.0.1:8000
ISPEC_VLLM_MODEL=allenai/Llama-3.1-Tulu-3-8B

# vLLM can be slow on first load / long prompts; keep this high for dev.
ISPEC_VLLM_TIMEOUT_SECONDS=300
